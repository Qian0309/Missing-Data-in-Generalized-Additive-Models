---
title: "GAMs project"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r,echo = FALSE}
library(mgcv)
library(mice)
library(missForest)
library(MASS)
library(ggplot2)
library(randomForest)
library(MVN)
library(Hotelling)
```

```{r code=readLines("code.R"), eval=TRUE, echo=FALSE}
# Do not change this code chunk
# Load function definitions
source("Gams function.R")
```

## data simulation
```{r data simulation,echo = FALSE}
set.seed(27)
error <- rnorm(n=1000,mean=0,sd=10)
covmatrix1 <- matrix(data=c(1,0,0,0,1,0,0,0,1),nrow = 3)
covmatrix2 <- matrix(data=c(rep(1,3),rep(2,3),rep(3,3)),nrow= 3)
data1_3 <- mvrnorm(n=1000,mu=c(5,5,5),Sigma=covmatrix1)
data4_6 <- mvrnorm(n=1000,mu=c(0,0.5,1),Sigma=covmatrix2)
data7 <- rexp(n=1000,rate=0.5)
data8 <- rpois(n=1000,lambda=3)
data9 <- rbinom(n=1000,size=10,prob=0.5)
data10 <- 2*data1_3[,1]+7*data4_6[,2]
data11 <- rgamma(n=1000,shape=1,rate = 1)
data12 <- rt(n=1000,df=10)
XData <- cbind(data1_3,data4_6,data7,data8,data9,data10,data11,data12)
M_Xdata <- as.matrix(XData)
beta <- matrix(data=c(sample(100:10000,12)/1000),ncol=1)
y <- M_Xdata %*% beta+error
Data <- cbind(y,XData)
colnames(Data) <- c("Y","X1","X2","X3","X4","X5","X6","X7","X8","X9","X10","X11","X12")
```

```{r}
dat <- gamSim(1,n=300,scale=3)
par(mfrow=c(1,4))
plot(f0~x0,data = dat)
plot(f1~x1,data = dat)
plot(f2~x2,data = dat)
plot(f3~x3,data = dat)
plot(x1~x2,data = dat)
plot(x1~x3,data = dat)
plot(x1~x0,data = dat)
```

```{r Simulate dataset}
error <- rnorm(300)*0.01
#x1 <- runif(300)
#x2 <- (x1-0.3)*(x1-0.5)*(x1-0.7)+rnorm(300)*0.01 #x2~x1
#x3 <- (x2-0.5)*(x2-0.5)+rnorm(300)*0.01 #x3~x2
#x4 <- (x3-0.2)*(x3-0.5)^2*(x3-0.8)+rnorm(300)*0.01 #x4~x3
#x5 <- 3*x1+rnorm(300)*0.01 #x5~x1
#x6 <- runif(300)

x1 <- runif(300)
x2 <- (x1-0.3)*(x1-0.5)*(x1-0.7)+rnorm(300)*0.01 #x2~x1
x3 <- (x2-0.5)*(x2-0.5)+rnorm(300)*0.01 #x3~x2
x4 <- (x3-0.2)*(x3-0.5)^2*(x3-0.8)+rnorm(300)*0.1 #x4~x3
x5 <- 3*x1+rnorm(300)*0.01 #x5~x1
x6 <- runif(300)
xdata <- cbind(x1,x2,x3,x4,x5,x6)
colnames(xdata) <- c("X1","X2","X3","X4","X5","X6")

par(mfrow=c(1,5))
plot(x1~x2)
plot(x2~x3)
plot(x3~x4)
plot(x1~x5)
plot(x6~x1)

#f1 <- (x1)^3
#f2 <- (x2)^2
#f3 <- x3
#f4 <- (x4-0.1)*(x4-0.3)^2*(x4-0.7)
#f5 <- (x5)^(1/2)
#f6 <- (x6-0.3)*(x6-0.5)*(x6-0.7)

#1st order
f1 <- x1
#2nd order
f2 <- 50*(3*x2-0.01)*(3*x2-0.05)
#3rd order
f3 <- 500*(x3-0.29)*(2*x3-0.32)*(1.5*x3-0.57)
#4th order
f4 <- 500*(x4+0.2)*(1.2*x4+0.075)*(1.2*x4-0.15)*(x4-0.24)
#5th order
f5 <- 10*(x5+0.1)*(x5-0.75)*(x5-1.25)*(x5-2.35)*(x5-3)
#6th order
f6 <- 1000*x6*(x6-0.17)*(x6-0.33)*(x6-0.59)*(x6-0.8)*(x6-0.95)

par(mfrow=c(2,3))
plot(f1~x1)
plot(f2~x2)
plot(f3~x3)
plot(f4~x4)
plot(f5~x5)
plot(f6~x6)

Y <- f1+f2+f3+f4+f5+f6+error
EY <- f1+f2+f3+f4+f5+f6
Data <- cbind(Y,x1,x2,x3,x4,x5,x6,f1,f2,f3,f4,f5,f6)
colnames(Data) <- c("Y","X1","X2","X3","X4","X5","X6","f1","f2","f3","f4","f5","f6")

#Test_rmse(Data,EY, "mean", 0.2, 5)

```

```{r Mean RMSE for each imputation method}
# Mean rmse for variables after MEAN imputation
mean_impute <- Test_rmse(Data,EY, "mean", 0.2, 5)
Mean_rmse <- as.matrix(colMeans(mean_impute[, 1:ncol(mean_impute)-1]))
colnames(Mean_rmse) <- "Root Mean Squared Error"
rownames(Mean_rmse) <- c("Y_rmse", "X1_rmse","X2_rmse","X3_rmse","X4_rmse","X5_rmse","X6_rmse")

# Mean rmse for variables after MICE imputation
mice_impute <- Test_rmse(Data,EY, "mice", 0.2, 5)
Mice_rmse <- as.matrix(colMeans(mice_impute[, 1:ncol(mice_impute)-1]))
colnames(Mice_rmse) <- "Root Mean Squared Error"
rownames(Mice_rmse) <- c("Y_rmse", "X1_rmse","X2_rmse","X3_rmse","X4_rmse","X5_rmse","X6_rmse")

# Mean rmse for variables after RANDOM FOREST imputation
rf_impute <- Test_rmse(Data,EY, "rf", 0.2, 5)
RF_rmse <- as.matrix(colMeans(rf_impute[, 1:ncol(rf_impute)-1]))
colnames(RF_rmse) <- "Root Mean Squared Error"
rownames(RF_rmse) <- c("Y_rmse", "X1_rmse","X2_rmse","X3_rmse","X4_rmse","X5_rmse","X6_rmse")
```

```{r PLot RMSE vs. Missing proportions of data}

# Create a list for several dropping proportions of the data
Missdata_prop_list <- c(0.2, 0.3, 0.4, 0.5, 0.6)
Missdata_prop_list <- as.matrix(Missdata_prop_list)

# Create a dataframe to store the mean RMSE for Random Forest Imputation method for each dropping proportions
df_mean_rmse <- data.frame(matrix(ncol = nrow(Missdata_prop_list), nrow = ncol(xdata)+1))
df_mice_rmse <- data.frame(matrix(ncol = nrow(Missdata_prop_list), nrow = ncol(xdata)+1))
df_rf_rmse <- data.frame(matrix(ncol = nrow(Missdata_prop_list), nrow = ncol(xdata)+1))

# Loop through each dropping proportions for all three imputation methods
for (i in 1:5){
  
  mean_missprop <- Test_rmse(Data,EY, "mean", 0.1*i, 2)
  df_mean_rmse[i] <- as.matrix(colMeans(mean_missprop[,1:ncol(mean_missprop)-1]))
  
  mice_missprop <- Test_rmse(Data,EY, "mice", 0.1*i, 2)
  df_mice_rmse[i] <- as.matrix(colMeans(mice_missprop[,1:ncol(mice_missprop)-1]))
  
  rf_missprop <- Test_rmse(Data,EY, "rf", 0.1*i, 2)
  df_rf_rmse[i] <- as.matrix(colMeans(rf_missprop[,1:ncol(rf_missprop)-1]))
  }

mean_Missprop <- t(df_mean_rmse)
mice_Missprop <- t(df_mice_rmse)
rf_Missprop <- t(df_rf_rmse)

matplot(Missdata_prop_list[,1], cbind(mean_Missprop[,1], mice_Missprop[,1], rf_Missprop[,1]), type = "l", col = c("blue", "red", "black"), xlab = "Missing Proportions of data", ylab = "Y_RMSE")

```

```{r Y_rmse vs. Number of Complete Cases}

# Y_rmse vs. Number of Complete Cases for MEAN Imputation
matplot(mean_impute[, ncol(mean_impute)], mean_impute[, 1], type = "p", xlab = "Complete Cases left after dropping data", ylab = "Y_RMSE")

# Y_rmse vs. Number of Complete Cases for MICE Imputation
matplot(mice_impute[, ncol(mice_impute)], mice_impute[, 1], type = "p", xlab = "Complete Cases left after dropping data", ylab = "Y_RMSE")

# Y_rmse vs. Number of Complete Cases for RF Imputation
matplot(rf_impute[, ncol(rf_impute)], rf_impute[, 1], type = "p", xlab = "Complete Cases left after dropping data", ylab = "Y_RMSE")
```



```{r Checking smooth functions with x_rmse after mean imputation}

loop <- 5
prop_miss <- 0.2

Data <- cbind(Y,x1,x2,x3,x4,x5,x6,f1,f2,f3,f4,f5,f6)
colnames(Data) <- c("Y","X1","X2","X3","X4","X5","X6","f1","f2","f3","f4","f5","f6")

# Define Y
Y <- Data[,1]
# Number of columns
# 1 Y, n xi's and n fx_i's (2n+1 columns in total)
N <- ncol(Data)
# Number of observations of each variable
r <- nrow(Data)
# locate the column of fx_1
col_f1 <- (N-1)/2+2
# Define all columns that containing fx_i's data
fx_Data <- Data[,col_f1:N]


# Define a dataset containing Y and x_i's, excluding all fx_i's columns
Data <- Data[,1:col_f1-1]
# Number of coulmns in dataframe "Data"
n <- ncol(Data)
# Define a dataset only containing x_i's, excluding all Y and fx_i's columns
xdata <- Data[,2:ncol(Data)]
colnames(xdata) <- c("X1","X2","X3","X4","X5","X6")

# Define matrix for storing Y_rmse, x_rmse and number of complete case
df_mean <- data.frame(matrix(ncol = n+1, nrow = loop))
df_mice <- data.frame(matrix(ncol = n+1, nrow = loop))
df_rf <- data.frame(matrix(ncol = n+1, nrow = loop))

# Apply GAMs to the non-dropping full dataframe
Data <- as.data.frame(Data)
O <- gam(Y~s(X1)+s(X2)+s(X3)+s(X4)+s(X5)+s(X6),data=Data,method="REML")
TO <- predict.gam(O,type="terms",as.data.frame(xdata))
pO <- predict.gam(O,as.data.frame(xdata))

# Compute root mean squared error for y variable and each x variables after imputing the missing data
# If the "Mean" imputation method was chosen

for (i in 1:loop){
  
  # Drop data
  Data.miss <- prodNA(xdata, noNA = prop_miss)
  completedcase <- nrow(na.omit(Data.miss))

  ## Apply "Mean" imputation method on dataframe with missing data
  mean_imputedData <- as.data.frame(Data.miss)
  for(j in 1:ncol(mean_imputedData)) {
    mean_imputedData[ , j][is.na(mean_imputedData[ , j])] <- mean(mean_imputedData[ , j], na.rm = TRUE)
  }
  as.matrix(mean_imputedData)
  
  # Apply GAMs to the full dataframe after mean imputation
  Mean_mod <- gam(Y~s(X1)+s(X2)+s(X3)+s(X4)+s(X5)+s(X6),data=mean_imputedData,method="REML")
 
  # GAMs prediction for each xi
  x_pred <- predict.gam(Mean_mod,type="terms",as.data.frame(xdata))

  # Rescale fx_i's
  re_fx <- rescale(fx_Data, x_pred)
  for (m in 1:ncol(re_fx)){
    re_fx[, m] <- c(re_fx[, m] - rep(colMeans(re_fx)[m], r))
  }
  
  # Compute root mean squared error (rmse) for each xi's
  x_rmse <- sqrt(colSums((x_pred-rescale(fx_Data, re_fx))^2)/r)
  
  #matplot(xdata[,1], cbind(TMean[,1],rescale(fx_Data[,1],TMean[,1])),col=c("red","green"),lty=c(1,1))
  
  # GAMs prediction for y
  Y_pred <- predict.gam(Mean_mod,as.data.frame(xdata))
  # Compute root mean squared error (rmse) for y
  Y_rmse <- c(sqrt(sum((Y_pred-EY)^2)/r))
  
  # Add the above computed x_rmse, y_rmse and the number of complete cases after dropping data
  # into the matrix previously created (eg. loop 1 results go into the first row of the matrix)
  df_mean[i,] <- c(Y_rmse,x_rmse,completedcase)
  
  # Add column names for each column in the dataframe "df_mice"
  colnames(df_mean) <- c("Y_rmse", "x1_rmse", "x2_rmse", "x3_rmse", "x4_rmse", "x5_rmse", "x6_rmse", "# Complete case")
    
  #par(mfrow = c(2, 6))  # Set up a 2 x 2 plotting space
  par(mfrow=c(2,3))
  
  # Plot smooth functions for xi's
  for (i in 1:ncol(xdata)){
    matplot(xdata[,i], cbind(x_pred[,i], re_fx[,i]), type = "p", col=c("red","green"), ylim = c(-5, 5))
    }
}

df_mean

```


```{r Checking smooth functions with x_rmse after mean imputation}

loop <- 5
prop_miss <- 0.2

Data <- cbind(Y,x1,x2,x3,x4,x5,x6,f1,f2,f3,f4,f5,f6)
colnames(Data) <- c("Y","X1","X2","X3","X4","X5","X6","f1","f2","f3","f4","f5","f6")

# Define Y
Y <- Data[,1]
# Number of columns
# 1 Y, n xi's and n fx_i's (2n+1 columns in total)
N <- ncol(Data)
# Number of observations of each variable
r <- nrow(Data)
# locate the column of fx_1
col_f1 <- (N-1)/2+2
# Define all columns that containing fx_i's data
fx_Data <- Data[,col_f1:N]

# Define a dataset containing Y and x_i's, excluding all fx_i's columns
Data <- Data[,1:col_f1-1]
# Number of coulmns in dataframe "Data"
n <- ncol(Data)
# Define a dataset only containing x_i's, excluding all Y and fx_i's columns
xdata <- Data[,2:ncol(Data)]
colnames(xdata) <- c("X1","X2","X3","X4","X5","X6")

# Define matrix for storing Y_rmse, x_rmse and number of complete case
df_mean <- data.frame(matrix(ncol = n+1, nrow = loop))
df_mice <- data.frame(matrix(ncol = n+1, nrow = loop))
df_rf <- data.frame(matrix(ncol = n+1, nrow = loop))

# Apply GAMs to the non-dropping full dataframe
Data <- as.data.frame(Data)
O <- gam(Y~s(X1)+s(X2)+s(X3)+s(X4)+s(X5)+s(X6),data=Data,method="REML")
TO <- predict.gam(O,type="terms",as.data.frame(xdata))
pO <- predict.gam(O,as.data.frame(xdata))

# Compute root mean squared error for y variable and each x variables after imputing the missing data
# If the "Mean" imputation method was chosen

for (i in 1:loop){
  
  # Drop data
  Data.miss <- prodNA(xdata, noNA = prop_miss)
  completedcase <- nrow(na.omit(Data.miss))

  ## Apply "Mice" imputation method on dataframe with missing data
  #You may skip removal of collinear variables at initialization by the remove_collinear flag.
  imputed_Data <- mice(Data.miss, method = 'pmm', diagnostics = FALSE , remove_collinear = FALSE, seed = 500)
  mice_imputedData <- as.data.frame(complete(imputed_Data))
  as.matrix(mice_imputedData)
 
  # Apply GAMs to the full dataframe after mice imputation
  Mice_mod <- gam(Y~s(X1)+s(X2)+s(X3)+s(X4)+s(X5)+s(X6),data=mice_imputedData,method="REML")
 
  # GAMs prediction for each xi
  x_pred <- predict.gam(Mice_mod,type="terms",as.data.frame(xdata))
  
    # Rescale fx_i's
  re_fx <- rescale(fx_Data, x_pred)
  for (m in 1:ncol(re_fx)){
    re_fx[, m] <- c(re_fx[, m] - rep(colMeans(re_fx)[m], r))
  }
  
  # Compute root mean squared error (rmse) for each xi's
  x_rmse <- sqrt(colSums((x_pred-rescale(fx_Data, re_fx))^2)/r)
  
  #matplot(xdata[,1], cbind(TMean[,1],rescale(fx_Data[,1],TMean[,1])),col=c("red","green"),lty=c(1,1))
  
  # GAMs prediction for y
  Y_pred <- predict.gam(Mice_mod,as.data.frame(xdata))
  # Compute root mean squared error (rmse) for y
  Y_rmse <- c(sqrt(sum((Y_pred-EY)^2)/r))
  
  # Add the above computed x_rmse, y_rmse and the number of complete cases after dropping data
  # into the matrix previously created (eg. loop 1 results go into the first row of the matrix)
  df_mice[i,] <- c(Y_rmse,x_rmse,completedcase)
  
  # Add column names for each column in the dataframe "df_mice"
  colnames(df_mice) <- c("Y_rmse", "x1_rmse", "x2_rmse", "x3_rmse", "x4_rmse", "x5_rmse", "x6_rmse", "# Complete case")
    
  #par(mfrow = c(2, 6))  # Set up a 2 x 2 plotting space
  par(mfrow=c(2,3))
  
  # Plot smooth functions for xi's
  for (i in 1:ncol(xdata)){
    matplot(xdata[,i], cbind(x_pred[,i], re_fx[,i]), type = "p", col=c("red","green"), ylim = c(-5, 5))
    }
}

df_mice

```




```{r Checking smooth functions with x_rmse after mean imputation}

loop <- 2
prop_miss <- 0.2

Data <- cbind(Y,x1,x2,x3,x4,x5,x6,f1,f2,f3,f4,f5,f6)
colnames(Data) <- c("Y","X1","X2","X3","X4","X5","X6","f1","f2","f3","f4","f5","f6")

# Define Y
Y <- Data[,1]
# Number of columns
# 1 Y, n xi's and n fx_i's (2n+1 columns in total)
N <- ncol(Data)
# Number of observations of each variable
r <- nrow(Data)
# locate the column of fx_1
col_f1 <- (N-1)/2+2
# Define all columns that containing fx_i's data
fx_Data <- Data[,col_f1:N]

# Define a dataset containing Y and x_i's, excluding all fx_i's columns
Data <- Data[,1:col_f1-1]
# Number of coulmns in dataframe "Data"
n <- ncol(Data)
# Define a dataset only containing x_i's, excluding all Y and fx_i's columns
xdata <- Data[,2:ncol(Data)]
colnames(xdata) <- c("X1","X2","X3","X4","X5","X6")

# Define matrix for storing Y_rmse, x_rmse and number of complete case
df_rf <- data.frame(matrix(ncol = n+1, nrow = loop))

# Apply GAMs to the non-dropping full dataframe
Data <- as.data.frame(Data)
O <- gam(Y~s(X1)+s(X2)+s(X3)+s(X4)+s(X5)+s(X6),data=Data,method="REML")
TO <- predict.gam(O,type="terms",as.data.frame(xdata))
pO <- predict.gam(O,as.data.frame(xdata))

# Compute root mean squared error for y variable and each x variables after imputing the missing data
# If the "Mean" imputation method was chosen

for (i in 1:loop){
  
  # Drop data
  Data.miss <- prodNA(xdata, noNA = prop_miss)
  completedcase <- nrow(na.omit(Data.miss))
  
  ## Apply "Random Forest" imputation method on dataframe with missing data
  rf_imputedData <- rfImpute(Y ~ ., Data.miss)[,2:n]
  as.matrix(rf_imputedData)
  
  # Apply GAMs to the full dataframe after mice imputation
  rf_mod <- gam(Y~s(X1)+s(X2)+s(X3)+s(X4)+s(X5)+s(X6),data=rf_imputedData,method="REML")

  # GAMs prediction for each xi
  x_pred <- predict.gam(rf_mod,type="terms",as.data.frame(xdata))
  
    # Rescale fx_i's
  re_fx <- rescale(fx_Data, x_pred)
  for (m in 1:ncol(re_fx)){
    re_fx[, m] <- c(re_fx[, m] - rep(colMeans(re_fx)[m], r))
  }
  
  # Compute root mean squared error (rmse) for each xi's
  x_rmse <- sqrt(colSums((x_pred-rescale(fx_Data, re_fx))^2)/r)
  
  # GAMs prediction for y
  Y_pred <- predict.gam(rf_mod,as.data.frame(xdata))
  # Compute root mean squared error (rmse) for y
  Y_rmse <- c(sqrt(sum((Y_pred-EY)^2)/r))
  
  # Add the above computed x_rmse, y_rmse and the number of complete cases after dropping data
  # into the matrix previously created (eg. loop 1 results go into the first row of the matrix)
  df_rf[i,] <- c(Y_rmse,x_rmse,completedcase)
  
  # Add column names for each column in the dataframe "df_mice"
  colnames(df_rf) <- c("Y_rmse", "x1_rmse", "x2_rmse", "x3_rmse", "x4_rmse", "x5_rmse", "x6_rmse", "# Complete case")
    
  #par(mfrow = c(2, 6))  # Set up a 2 x 2 plotting space
  par(mfrow=c(2,3))
  
  # Plot smooth functions for xi's
  for (i in 1:ncol(xdata)){
    matplot(xdata[,i], cbind(x_pred[,i], re_fx[,i]), type = "p", col=c("red","green"), ylim = c(-5, 5))
    }
}

df_rf

```


```{r test mvn and mean 删除}
##
#H0 (null): The variables not follow a multivariate normal distribution.
#Ha (alternative): The variables follow a multivariate normal distribution.
#result <- mvn(xdata,mvnTest = "hz")
#result1 <- mvn(ms_completeData,mvnTest = "hz")
#result2 <- mvn(mi_completeData,mvnTest = "hz")
#result3 <- mvn(rf_completeData,mvnTest = "hz")
#result$multivariateNormality$MVN
#result1$multivariateNormality$MVN
#result2$multivariateNormality$MVN
#result3$multivariateNormality$MVN

#hoteling t square distribution test
#H0 mu same
#Ha mu not same
#t1 <- hotelling.stat(xdata,as.matrix(ms_completeData))$statistic
#t2 <- hotelling.stat(xdata,as.matrix(mi_completeData))$statistic
#t3 <- hotelling.stat(xdata,as.matrix(rf_completeData))$statistic
```


